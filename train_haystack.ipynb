{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación y entrenamiento de un sistema de preguntas y respuestas utilizando Haystack\n",
    "\n",
    "**Tabla de contenidos:**\n",
    "\n",
    "1. [División del dataset](#división-del-dataset)\n",
    "2. [Importar las bibliotecas necesarias y definir modelos](#Importar-las-bibliotecas-necesarias-y-definir-modelos)\n",
    "3. [Definición de las funciones auxiliares](#Definición-de-las-funciones-auxiliares)\n",
    "4. [Preparación de los datos](#Preparación-de-los-datos)\n",
    "5. [Uso de la pipeline para hacer preguntas](#Uso-de-la-pipeline-para-hacer-preguntas)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División del dataset\n",
    "\n",
    "El código proporcionado en el script \"json_split.py\" toma un archivo JSON de SQuAD como entrada y divide el conjunto de datos en tres partes: entrenamiento, desarrollo y validación, con proporciones de 80%, 10% y 10%, respectivamente. Luego, guarda estos conjuntos divididos en archivos JSON independientes.\n",
    "\n",
    "- El archivo de entrada es un archivo JSON de SQuAD ubicado en ['./data/data.json'](https://github.com/josuemzx/Chatbot-para-refugiados/blob/main/data/Dataset%20propio/data.json), que se puede encontrar en el repositorio de GitHub [Chatbot-para-refugiados](https://github.com/josuemzx/Chatbot-para-refugiados/).\n",
    "- La función split_squad_data se encarga de dividir el conjunto de datos en las proporciones especificadas. Esta función acepta un archivo JSON de entrada y proporciones opcionales para entrenamiento y desarrollo (por defecto, 0.8 y 0.1).\n",
    "- Después de dividir los datos, el código guarda los conjuntos divididos en archivos JSON:\n",
    "    - El conjunto de entrenamiento se guarda en 'data/train.json'.\n",
    "    - El conjunto de desarrollo se guarda en 'data/dev.json'.\n",
    "    - El conjunto de validación se guarda en 'data/val.json'.\n",
    "\n",
    "Esta división permite entrenar, ajustar y evaluar modelos de aprendizaje automático utilizando diferentes subconjuntos del conjunto de datos original, lo que ayuda a evitar el sobreajuste y a garantizar que el modelo generalice bien a datos no vistos previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python json_split.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar las bibliotecas necesarias y definir modelos\n",
    "\n",
    "En esta sección, importamos las bibliotecas necesarias para la creación y entrenamiento del sistema de preguntas y respuestas. Además, definimos los modelos pre-entrenados en español que vamos a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import FARMReader, DensePassageRetriever, PreProcessor\n",
    "from haystack.utils import convert_files_to_docs\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "spanish_models = [\n",
    "    \"deepset/xlm-roberta-base-squad2-distilled\",\n",
    "    \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
    "    \"dccuchile/albert-base-10-spanish-finetuned-qa-sqac\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de las funciones auxiliares\n",
    "\n",
    "Aquí definimos las funciones auxiliares que se utilizarán para crear el document store, el retriever, entrenar el reader y crear la pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_store(docs: List[str], custom_db_filename: str) -> FAISSDocumentStore:\n",
    "    # Crear objeto FAISSDocumentStore y cargar documentos\n",
    "    # Crear una carpeta si no existe. Aconsejo borrarla si ya existe para que no dé error.\n",
    "    folder_name = \"databases\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    document_store = FAISSDocumentStore(sql_url=f\"sqlite:///{folder_name}/{custom_db_filename}\")\n",
    "    document_store.write_documents(docs)\n",
    "    return document_store\n",
    "\n",
    "def create_retriever(document_store: FAISSDocumentStore, model: str) -> DensePassageRetriever:\n",
    "    # Crear objeto DensePassageRetriever\n",
    "    retriever = DensePassageRetriever(\n",
    "        document_store=document_store,\n",
    "        query_embedding_model=model,\n",
    "        passage_embedding_model=model,\n",
    "        use_gpu=True\n",
    "    )\n",
    "    return retriever\n",
    "\n",
    "def train_reader(model: str, dataset_path: str, save_dir: str) -> FARMReader:\n",
    "    # Crear objeto FARMReader para entrenar el modelo DPR con el archivo SQuAD\n",
    "    reader = FARMReader(\n",
    "        model_name_or_path=model,\n",
    "        use_gpu=True,\n",
    "        context_window_size=100,\n",
    "        top_k_per_candidate=3,\n",
    "        return_no_answer=False,\n",
    "        num_processes=0\n",
    "    )\n",
    "    reader.train(\n",
    "        data_dir=dataset_path,\n",
    "        train_filename='train.json',\n",
    "        dev_filename='dev.json',\n",
    "        test_filename='val.json',\n",
    "        use_gpu=True,\n",
    "        batch_size=8,\n",
    "        n_epochs=25,\n",
    "        learning_rate=2e-5,\n",
    "        evaluate_every=80\n",
    "    )\n",
    "    return reader\n",
    "\n",
    "def create_pipeline(reader: FARMReader, retriever: DensePassageRetriever) -> ExtractiveQAPipeline:\n",
    "    # Crear objeto ExtractiveQAPipeline que use el document_store y el FARMReader entrenado para responder a las preguntas\n",
    "    pipeline = ExtractiveQAPipeline(\n",
    "        reader=reader,\n",
    "        retriever=retriever\n",
    "    )\n",
    "    return pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos\n",
    "\n",
    "En esta sección, preparamos los datos para su procesamiento, convirtiendo los archivos en documentos y aplicando un preprocesamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas a los archivos y modelo\n",
    "docs_path = \"data/docs/\"\n",
    "dataset_path = \"data/\"\n",
    "save_dir = \"trained_reader/\"\n",
    "\n",
    "# Convertir archivos en documentos y procesarlos con el PreProcessor\n",
    "all_docs = convert_files_to_docs(dir_path=docs_path)\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=100,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "docs = preprocessor.process(all_docs)\n",
    "\n",
    "pipes = []\n",
    "for model, i in zip(spanish_models, range(len(spanish_models))):\n",
    "\n",
    "    # Crear document_store y cargar documentos\n",
    "    document_store = create_document_store(docs, str(i)+'_db')\n",
    "\n",
    "    # Crear DensePassageRetriever y actualizar embeddings en el document_store\n",
    "    retriever = create_retriever(document_store, model)\n",
    "    document_store.update_embeddings(retriever)\n",
    "    # Crear una carpeta si no existe\n",
    "    folder_name = \"retrievers\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    retriever.save(folder_name+'/'+str(i))\n",
    "\n",
    "    # Entrenar FARMReader con el conjunto de datos y guardar el modelo\n",
    "    reader = train_reader(model, dataset_path, save_dir)\n",
    "    # Crear una carpeta si no existe\n",
    "    folder_name = \"readers\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    reader.save(folder_name+'/'+str(i))\n",
    "    \n",
    "    print(\"Entrenamiento finalizado.\")\n",
    "\n",
    "    # Crear ExtractiveQAPipeline con el FARMReader entrenado y el DensePassageRetriever\n",
    "    pipeline = create_pipeline(reader, retriever)\n",
    "    pipes.append(pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de la pipeline para hacer preguntas\n",
    "\n",
    "Finalmente, en esta sección, utilizamos la pipeline para realizar preguntas y obtener respuestas de los documentos procesados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora puedes usar la pipeline para hacer preguntas, por ejemplo:\n",
    "question = \"¿Dónde acudir al llegar a España?\"\n",
    "for pipe in pipes:\n",
    "    result = pipe.run(query=question)['answers'][0].answer\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook te guía a través de la creación y entrenamiento de un sistema de preguntas y respuestas utilizando el framework Haystack. Con este sistema, puedes buscar información relevante en un conjunto de documentos y obtener respuestas precisas a tus preguntas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
